{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea80ec3-e7ce-49f0-823b-5067a52aea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.10) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from itertools import chain\n",
    "import gensim\n",
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5578b3a3-b9bf-43eb-a89f-d4ff168904df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as mds\n",
    "def create_dataset(base_path, batch_size, num_epochs, is_train):\n",
    "    columns_list = [\"feature\", \"label\"]\n",
    "    num_consumer = 4\n",
    "    if is_train:\n",
    "        path = os.path.join(base_path, \"aclImdb_train.mindrecord0\")\n",
    "    else:\n",
    "        path = os.path.join(base_path, \"aclImdb_test.mindrecord0\")\n",
    "    dataset = mds.MindDataset(path, columns_list=[\"feature\", \"label\"], num_parallel_workers=4)\n",
    "    dataset = dataset.shuffle(buffer_size=dataset.get_dataset_size())\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(count=num_epochs)\n",
    "    return dataset\n",
    "dataset_train = create_dataset(\"./mindrecord\", batch_size=32, num_epochs=10, is_train=True)\n",
    "dataset_real_test=create_dataset(\"./mindrecordtest\", batch_size=32, num_epochs=10, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfd986b-6a42-49dc-8836-c5fdd56c31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tabel = np.loadtxt(os.path.join(\"./embedding/\", \"weight.txt\")).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d75ccd-f538-485e-8e3d-d362f945ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Huawei Technologies Co., Ltd\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\"\"\"LSTM.\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import Tensor, nn, context, Parameter, ParameterTuple\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.ops import operations as P\n",
    "\n",
    "STACK_LSTM_DEVICE = [\"CPU\"]\n",
    "\n",
    "\n",
    "# Initialize short-term memory (h) and long-term memory (c) to 0\n",
    "def lstm_default_state(batch_size, hidden_size, num_layers, bidirectional):\n",
    "    \"\"\"init default input.\"\"\"\n",
    "    num_directions = 2 if bidirectional else 1\n",
    "    h = Tensor(np.zeros((num_layers * num_directions, batch_size, hidden_size)).astype(np.float32))\n",
    "    c = Tensor(np.zeros((num_layers * num_directions, batch_size, hidden_size)).astype(np.float32))\n",
    "    return h, c\n",
    "\n",
    "\n",
    "def stack_lstm_default_state(batch_size, hidden_size, num_layers, bidirectional):\n",
    "    \"\"\"init default input.\"\"\"\n",
    "    num_directions = 2 if bidirectional else 1\n",
    "\n",
    "    h_list = c_list = []\n",
    "    for _ in range(num_layers):\n",
    "        h_list.append(Tensor(np.zeros((num_directions, batch_size, hidden_size)).astype(np.float32)))\n",
    "        c_list.append(Tensor(np.zeros((num_directions, batch_size, hidden_size)).astype(np.float32)))\n",
    "    h, c = tuple(h_list), tuple(c_list)\n",
    "    return h, c\n",
    "\n",
    "\n",
    "class StackLSTM(nn.Cell):\n",
    "    \"\"\"\n",
    "    Stack multi-layers LSTM together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1,\n",
    "                 has_bias=True,\n",
    "                 batch_first=False,\n",
    "                 dropout=0.0,\n",
    "                 bidirectional=False):\n",
    "        super(StackLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.transpose = P.Transpose()\n",
    "\n",
    "        # direction number\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # input_size list\n",
    "        input_size_list = [input_size]\n",
    "        for i in range(num_layers - 1):\n",
    "            input_size_list.append(hidden_size * num_directions)\n",
    "\n",
    "        # layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.LSTM(input_size=input_size_list[i],\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      has_bias=has_bias,\n",
    "                                      batch_first=batch_first,\n",
    "                                      bidirectional=bidirectional,\n",
    "                                      dropout=dropout))\n",
    "\n",
    "        # weights\n",
    "        weights = []\n",
    "        for i in range(num_layers):\n",
    "            # weight size\n",
    "            weight_size = (input_size_list[i] + hidden_size) * num_directions * hidden_size * 4\n",
    "            if has_bias:\n",
    "                bias_size = num_directions * hidden_size * 4\n",
    "                weight_size = weight_size + bias_size\n",
    "\n",
    "            # numpy weight\n",
    "            stdv = 1 / math.sqrt(hidden_size)\n",
    "            w_np = np.random.uniform(-stdv, stdv, (weight_size, 1, 1)).astype(np.float32)\n",
    "\n",
    "            # lstm weight\n",
    "            weights.append(Parameter(initializer(Tensor(w_np), w_np.shape), name=\"weight\" + str(i)))\n",
    "\n",
    "        #\n",
    "        self.lstms = layers\n",
    "        self.weight = ParameterTuple(tuple(weights))\n",
    "\n",
    "    def construct(self, x, hx):\n",
    "        \"\"\"construct\"\"\"\n",
    "        if self.batch_first:\n",
    "            x = self.transpose(x, (1, 0, 2))\n",
    "        # stack lstm\n",
    "        h, c = hx\n",
    "        hn = cn = None\n",
    "        for i in range(self.num_layers):\n",
    "            x, hn, cn, _, _ = self.lstms[i](x, h[i], c[i], self.weight[i])\n",
    "        if self.batch_first:\n",
    "            x = self.transpose(x, (1, 0, 2))\n",
    "        return x, (hn, cn)\n",
    "\n",
    "\n",
    "class SentimentNet(nn.Cell):\n",
    "    \"\"\"Sentiment network structure.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_size,\n",
    "                 num_hiddens,\n",
    "                 num_layers,\n",
    "                 bidirectional,\n",
    "                 num_classes,\n",
    "                 weight,\n",
    "                 batch_size):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        # Mapp words to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_size,\n",
    "                                      embedding_table=weight)\n",
    "        self.embedding.embedding_table.requires_grad = False\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "\n",
    "        if context.get_context(\"device_target\") in STACK_LSTM_DEVICE:\n",
    "            # stack lstm by user\n",
    "            self.encoder = StackLSTM(input_size=embed_size,\n",
    "                                     hidden_size=num_hiddens,\n",
    "                                     num_layers=num_layers,\n",
    "                                     has_bias=True,\n",
    "                                     bidirectional=bidirectional,\n",
    "                                     dropout=0.0)\n",
    "            self.h, self.c = stack_lstm_default_state(batch_size, num_hiddens, num_layers, bidirectional)\n",
    "        else:\n",
    "            # standard lstm\n",
    "            self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                                   hidden_size=num_hiddens,\n",
    "                                   num_layers=num_layers,\n",
    "                                   has_bias=True,\n",
    "                                   bidirectional=bidirectional,\n",
    "                                   dropout=0.0)\n",
    "            self.h, self.c = lstm_default_state(batch_size, num_hiddens, num_layers, bidirectional)\n",
    "\n",
    "        self.concat = P.Concat(1)\n",
    "        if bidirectional:\n",
    "            self.decoder = nn.Dense(num_hiddens * 4, num_classes)\n",
    "        else:\n",
    "            self.decoder = nn.Dense(num_hiddens * 2, num_classes)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        # inputï¼š(64,500,300)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, _ = self.encoder(embeddings, (self.h, self.c))\n",
    "        # states[i] size(64,200)  -> encoding.size(64,400)\n",
    "        encoding = self.concat((output[0], output[499]))\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed03693-d444-4758-b612-c7d992c2fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Tensor, nn, Model, context, Parameter\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.nn import Accuracy\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e18191d-2bae-4075-879a-4a8faaffc4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(36933,ffff9d49e780,python):2023-01-15-21:07:55.549.265 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(36933,ffff9d49e780,python):2023-01-15-21:07:55.549.861 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 60, loss is 0.7171497941017151\n",
      "epoch: 1 step: 120, loss is 0.7218549251556396\n",
      "epoch: 1 step: 180, loss is 0.776342511177063\n",
      "epoch: 1 step: 240, loss is 0.6117372512817383\n",
      "epoch: 1 step: 300, loss is 0.9299334287643433\n",
      "epoch: 1 step: 360, loss is 0.8461623191833496\n",
      "epoch: 1 step: 420, loss is 0.6753621101379395\n",
      "epoch: 1 step: 480, loss is 1.687245488166809\n",
      "epoch: 1 step: 540, loss is 0.6154614686965942\n",
      "epoch: 1 step: 600, loss is 0.5699261426925659\n",
      "epoch: 1 step: 660, loss is 0.6977465152740479\n",
      "epoch: 1 step: 720, loss is 1.0413440465927124\n",
      "epoch: 1 step: 780, loss is 0.6589983701705933\n",
      "epoch: 1 step: 840, loss is 0.6349864602088928\n",
      "epoch: 1 step: 900, loss is 0.7926942110061646\n",
      "epoch: 1 step: 960, loss is 0.7042611837387085\n",
      "epoch: 1 step: 1020, loss is 1.0392495393753052\n",
      "epoch: 1 step: 1080, loss is 0.6065108776092529\n",
      "epoch: 1 step: 1140, loss is 0.6942701935768127\n",
      "epoch: 1 step: 1200, loss is 0.6698892116546631\n",
      "epoch: 1 step: 1260, loss is 0.7141430377960205\n",
      "epoch: 1 step: 1320, loss is 0.6391139030456543\n",
      "epoch: 1 step: 1380, loss is 0.6512590646743774\n",
      "epoch: 1 step: 1440, loss is 0.6930268406867981\n",
      "epoch: 1 step: 1500, loss is 1.121873378753662\n",
      "epoch: 1 step: 1560, loss is 1.2271528244018555\n",
      "epoch: 1 step: 1620, loss is 0.6227319240570068\n",
      "epoch: 1 step: 1680, loss is 0.6857776641845703\n",
      "epoch: 1 step: 1740, loss is 0.53759765625\n",
      "epoch: 1 step: 1800, loss is 1.084630012512207\n",
      "epoch: 1 step: 1860, loss is 1.1485730409622192\n",
      "epoch: 1 step: 1920, loss is 1.2796571254730225\n",
      "epoch: 1 step: 1980, loss is 1.362971305847168\n",
      "epoch: 1 step: 2040, loss is 0.7576533555984497\n",
      "epoch: 1 step: 2100, loss is 0.6198320388793945\n",
      "epoch: 1 step: 2160, loss is 0.5971055030822754\n",
      "epoch: 1 step: 2220, loss is 0.9287587404251099\n",
      "epoch: 1 step: 2280, loss is 0.8609119653701782\n",
      "epoch: 1 step: 2340, loss is 0.6927806735038757\n",
      "epoch: 1 step: 2400, loss is 0.6882563829421997\n",
      "epoch: 1 step: 2460, loss is 0.9943827986717224\n",
      "epoch: 1 step: 2520, loss is 0.8179537653923035\n",
      "epoch: 1 step: 2580, loss is 0.7345936298370361\n",
      "epoch: 1 step: 2640, loss is 0.7324645519256592\n",
      "epoch: 1 step: 2700, loss is 0.9288851022720337\n"
     ]
    }
   ],
   "source": [
    "# from mindspore.model_zoo.lstm import SentimentNet\n",
    "\n",
    "network = SentimentNet(vocab_size=embedding_tabel.shape[0],\n",
    "                embed_size=100,\n",
    "                num_hiddens=100,\n",
    "                num_layers=2,\n",
    "                bidirectional=False,\n",
    "                num_classes=2,\n",
    "                weight=Tensor(embedding_tabel),\n",
    "                batch_size=32)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits( sparse=True)\n",
    "opt = nn.Momentum(network.trainable_params(), 0.1, 0.9)\n",
    "loss_callback = LossMonitor(per_print_times=60)\n",
    "model = Model(network, loss, opt, {'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=390, keep_checkpoint_max=10)\n",
    "checkpoint_cb = ModelCheckpoint(prefix=\"lstm\", directory=\"./model\", config=config_ck)\n",
    "\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"Ascend\")\n",
    "model.train(1, dataset_train, callbacks=[checkpoint_cb, loss_callback], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf61adbb-8da3-4856-ab64-f72fc8b37d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:04:25.926.483 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:04:25.927.125 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 60, loss is 0.6436595916748047\n",
      "epoch: 1 step: 120, loss is 0.6030970811843872\n",
      "epoch: 1 step: 180, loss is 0.6438596844673157\n",
      "epoch: 1 step: 240, loss is 0.7073056697845459\n",
      "epoch: 1 step: 300, loss is 0.5240635871887207\n",
      "epoch: 1 step: 360, loss is 0.6252241134643555\n",
      "epoch: 1 step: 420, loss is 0.6011233329772949\n",
      "epoch: 1 step: 480, loss is 0.6071441173553467\n",
      "epoch: 1 step: 540, loss is 0.6463255286216736\n",
      "epoch: 1 step: 600, loss is 0.5728106498718262\n",
      "epoch: 1 step: 660, loss is 0.5444440841674805\n",
      "epoch: 1 step: 720, loss is 0.6817660331726074\n",
      "epoch: 1 step: 780, loss is 0.68641197681427\n",
      "epoch: 1 step: 840, loss is 0.6923555731773376\n",
      "epoch: 1 step: 900, loss is 0.7211882472038269\n",
      "epoch: 1 step: 960, loss is 0.5435845255851746\n",
      "epoch: 1 step: 1020, loss is 0.5553909540176392\n",
      "epoch: 1 step: 1080, loss is 0.5065057277679443\n",
      "epoch: 1 step: 1140, loss is 0.5389652848243713\n",
      "epoch: 1 step: 1200, loss is 0.6151940822601318\n",
      "epoch: 1 step: 1260, loss is 0.5626600980758667\n",
      "epoch: 1 step: 1320, loss is 0.5505446195602417\n",
      "epoch: 1 step: 1380, loss is 0.6897614002227783\n",
      "epoch: 1 step: 1440, loss is 0.5950238704681396\n",
      "epoch: 1 step: 1500, loss is 0.5648951530456543\n",
      "epoch: 1 step: 1560, loss is 0.6331925392150879\n",
      "epoch: 1 step: 1620, loss is 0.5800169110298157\n",
      "epoch: 1 step: 1680, loss is 0.5077096223831177\n",
      "epoch: 1 step: 1740, loss is 0.5332719087600708\n",
      "epoch: 1 step: 1800, loss is 0.4738175868988037\n",
      "epoch: 1 step: 1860, loss is 0.5889996290206909\n",
      "epoch: 1 step: 1920, loss is 0.6527118682861328\n",
      "epoch: 1 step: 1980, loss is 0.5590983033180237\n",
      "epoch: 1 step: 2040, loss is 0.5951193571090698\n",
      "epoch: 1 step: 2100, loss is 0.4930175542831421\n",
      "epoch: 1 step: 2160, loss is 0.6281660795211792\n",
      "epoch: 1 step: 2220, loss is 0.6642143726348877\n",
      "epoch: 1 step: 2280, loss is 0.5407525897026062\n",
      "epoch: 1 step: 2340, loss is 0.5740801095962524\n",
      "epoch: 1 step: 2400, loss is 0.5574960708618164\n",
      "epoch: 1 step: 2460, loss is 0.7013329863548279\n",
      "epoch: 1 step: 2520, loss is 0.4185504913330078\n",
      "epoch: 1 step: 2580, loss is 0.5820934772491455\n",
      "epoch: 1 step: 2640, loss is 0.6366661787033081\n",
      "epoch: 1 step: 2700, loss is 0.7643030285835266\n"
     ]
    }
   ],
   "source": [
    "network = SentimentNet(vocab_size=embedding_tabel.shape[0],\n",
    "                embed_size=100,\n",
    "                num_hiddens=100,\n",
    "                num_layers=2,\n",
    "                bidirectional=False,\n",
    "                num_classes=2,\n",
    "                weight=Tensor(embedding_tabel),\n",
    "                batch_size=32)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits( sparse=True)\n",
    "opt = nn.Adam(network.trainable_params(), learning_rate=1e-3, weight_decay=1e-6)\n",
    "loss_callback = LossMonitor(per_print_times=60)\n",
    "model = Model(network, loss, opt, {'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=390, keep_checkpoint_max=10)\n",
    "checkpoint_cb = ModelCheckpoint(prefix=\"lstm\", directory=\"./model\", config=config_ck)\n",
    "\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"Ascend\")\n",
    "model.train(1, dataset_train, callbacks=[checkpoint_cb, loss_callback], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49ca67b-c340-406e-a3a7-7f9cdb44e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:15:37.464.265 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:15:37.464.816 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 60, loss is 0.6783525943756104\n",
      "epoch: 1 step: 120, loss is 0.6577904224395752\n",
      "epoch: 1 step: 180, loss is 0.6689574718475342\n",
      "epoch: 1 step: 240, loss is 0.6629035472869873\n",
      "epoch: 1 step: 300, loss is 0.6530580520629883\n",
      "epoch: 1 step: 360, loss is 0.6137760877609253\n",
      "epoch: 1 step: 420, loss is 0.652942955493927\n",
      "epoch: 1 step: 480, loss is 0.734551191329956\n",
      "epoch: 1 step: 540, loss is 0.6338605880737305\n",
      "epoch: 1 step: 600, loss is 0.6435796618461609\n",
      "epoch: 1 step: 660, loss is 0.6808139681816101\n",
      "epoch: 1 step: 720, loss is 0.6622568368911743\n",
      "epoch: 1 step: 780, loss is 0.6049716472625732\n",
      "epoch: 1 step: 840, loss is 0.5817291736602783\n",
      "epoch: 1 step: 900, loss is 0.6636234521865845\n",
      "epoch: 1 step: 960, loss is 0.6992130279541016\n",
      "epoch: 1 step: 1020, loss is 0.6433334350585938\n",
      "epoch: 1 step: 1080, loss is 0.6429691314697266\n",
      "epoch: 1 step: 1140, loss is 0.6113216876983643\n",
      "epoch: 1 step: 1200, loss is 0.6428802609443665\n",
      "epoch: 1 step: 1260, loss is 0.6140112280845642\n",
      "epoch: 1 step: 1320, loss is 0.6150246858596802\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "dataset_train = create_dataset(\"./mindrecord\", batch_size=batch_size, num_epochs=10, is_train=True)\n",
    "network = SentimentNet(vocab_size=embedding_tabel.shape[0],\n",
    "                embed_size=100,\n",
    "                num_hiddens=100,\n",
    "                num_layers=2,\n",
    "                bidirectional=False,\n",
    "                num_classes=2,\n",
    "                weight=Tensor(embedding_tabel),\n",
    "                batch_size=batch_size)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits( sparse=True)\n",
    "opt = nn.Adam(network.trainable_params(), learning_rate=1e-4, weight_decay=1e-6)\n",
    "loss_callback = LossMonitor(per_print_times=60)\n",
    "model = Model(network, loss, opt, {'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=390, keep_checkpoint_max=10)\n",
    "checkpoint_cb = ModelCheckpoint(prefix=\"lstm\", directory=\"./model\", config=config_ck)\n",
    "\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"Ascend\")\n",
    "model.train(1, dataset_train, callbacks=[checkpoint_cb, loss_callback], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bad5b77-9a41-4f83-896c-fe4f84a000a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:47:12.798.480 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(2100,ffff9f60f780,python):2023-01-15-22:47:12.799.003 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 60, loss is 0.663476288318634\n",
      "epoch: 1 step: 120, loss is 0.6531549692153931\n",
      "epoch: 1 step: 180, loss is 0.694236159324646\n",
      "epoch: 1 step: 240, loss is 0.6823680400848389\n",
      "epoch: 1 step: 300, loss is 0.6060357689857483\n",
      "epoch: 1 step: 360, loss is 0.6931272149085999\n",
      "epoch: 1 step: 420, loss is 0.6161444187164307\n",
      "epoch: 1 step: 480, loss is 0.6625012159347534\n",
      "epoch: 1 step: 540, loss is 0.6367723941802979\n",
      "epoch: 1 step: 600, loss is 0.664122462272644\n",
      "epoch: 1 step: 660, loss is 0.6331920027732849\n",
      "epoch: 1 step: 720, loss is 0.6884821057319641\n",
      "epoch: 1 step: 780, loss is 0.6616216897964478\n",
      "epoch: 1 step: 840, loss is 0.6724331378936768\n",
      "epoch: 1 step: 900, loss is 0.5720317363739014\n",
      "epoch: 1 step: 960, loss is 0.6618954539299011\n",
      "epoch: 1 step: 1020, loss is 0.6544455289840698\n",
      "epoch: 1 step: 1080, loss is 0.6604650020599365\n",
      "epoch: 1 step: 1140, loss is 0.6418930292129517\n",
      "epoch: 1 step: 1200, loss is 0.6735744476318359\n",
      "epoch: 1 step: 1260, loss is 0.6695458889007568\n",
      "epoch: 1 step: 1320, loss is 0.6414956450462341\n",
      "epoch: 2 step: 10, loss is 0.7021751403808594\n",
      "epoch: 2 step: 70, loss is 0.6743639707565308\n",
      "epoch: 2 step: 130, loss is 0.6659883260726929\n",
      "epoch: 2 step: 190, loss is 0.6305171847343445\n",
      "epoch: 2 step: 250, loss is 0.6216598153114319\n",
      "epoch: 2 step: 310, loss is 0.6254526972770691\n",
      "epoch: 2 step: 370, loss is 0.6164652109146118\n",
      "epoch: 2 step: 430, loss is 0.6034106612205505\n",
      "epoch: 2 step: 490, loss is 0.6348305344581604\n",
      "epoch: 2 step: 550, loss is 0.5990987420082092\n",
      "epoch: 2 step: 610, loss is 0.5758699178695679\n",
      "epoch: 2 step: 670, loss is 0.5972522497177124\n",
      "epoch: 2 step: 730, loss is 0.5803312063217163\n",
      "epoch: 2 step: 790, loss is 0.6379673480987549\n",
      "epoch: 2 step: 850, loss is 0.654775857925415\n",
      "epoch: 2 step: 910, loss is 0.5769396424293518\n",
      "epoch: 2 step: 970, loss is 0.5620162487030029\n",
      "epoch: 2 step: 1030, loss is 0.6408800482749939\n",
      "epoch: 2 step: 1090, loss is 0.6265532970428467\n",
      "epoch: 2 step: 1150, loss is 0.6432105302810669\n",
      "epoch: 2 step: 1210, loss is 0.6474442481994629\n",
      "epoch: 2 step: 1270, loss is 0.6004011631011963\n",
      "epoch: 2 step: 1330, loss is 0.5977554321289062\n",
      "epoch: 3 step: 20, loss is 0.6125681400299072\n",
      "epoch: 3 step: 80, loss is 0.6301037073135376\n",
      "epoch: 3 step: 140, loss is 0.6109367609024048\n",
      "epoch: 3 step: 200, loss is 0.6578718423843384\n",
      "epoch: 3 step: 260, loss is 0.6208117008209229\n",
      "epoch: 3 step: 320, loss is 0.5917845964431763\n",
      "epoch: 3 step: 380, loss is 0.5947350263595581\n",
      "epoch: 3 step: 440, loss is 0.63288414478302\n",
      "epoch: 3 step: 500, loss is 0.646415114402771\n",
      "epoch: 3 step: 560, loss is 0.6001901030540466\n",
      "epoch: 3 step: 620, loss is 0.6471445560455322\n",
      "epoch: 3 step: 680, loss is 0.5860140323638916\n",
      "epoch: 3 step: 740, loss is 0.6470317840576172\n",
      "epoch: 3 step: 800, loss is 0.5818643569946289\n",
      "epoch: 3 step: 860, loss is 0.5183252096176147\n",
      "epoch: 3 step: 920, loss is 0.5717165470123291\n",
      "epoch: 3 step: 980, loss is 0.6791113615036011\n",
      "epoch: 3 step: 1040, loss is 0.5598769187927246\n",
      "epoch: 3 step: 1100, loss is 0.6746722459793091\n",
      "epoch: 3 step: 1160, loss is 0.6422296762466431\n",
      "epoch: 3 step: 1220, loss is 0.5949664115905762\n",
      "epoch: 3 step: 1280, loss is 0.5762118101119995\n",
      "epoch: 3 step: 1340, loss is 0.6276646852493286\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "dataset_train = create_dataset(\"./mindrecord\", batch_size=batch_size, num_epochs=10, is_train=True)\n",
    "network = SentimentNet(vocab_size=embedding_tabel.shape[0],\n",
    "                embed_size=100,\n",
    "                num_hiddens=100,\n",
    "                num_layers=2,\n",
    "                bidirectional=False,\n",
    "                num_classes=2,\n",
    "                weight=Tensor(embedding_tabel),\n",
    "                batch_size=batch_size)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits( sparse=True)\n",
    "opt = nn.Adam(network.trainable_params(), learning_rate=2*1e-4, weight_decay=1e-6)\n",
    "loss_callback = LossMonitor(per_print_times=60)\n",
    "model = Model(network, loss, opt, {'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=390, keep_checkpoint_max=10)\n",
    "checkpoint_cb = ModelCheckpoint(prefix=\"lstm\", directory=\"./model\", config=config_ck)\n",
    "\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"Ascend\")\n",
    "model.train(3, dataset_train, callbacks=[checkpoint_cb, loss_callback], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0296f971-0223-4b1b-9c52-7acedd3d2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(72133,ffff8505d780,python):2023-01-15-23:07:38.032.688 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(72133,ffff8505d780,python):2023-01-15-23:07:38.033.272 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 60, loss is 0.6922612190246582\n",
      "epoch: 1 step: 120, loss is 0.6918786764144897\n",
      "epoch: 1 step: 180, loss is 0.6904754638671875\n",
      "epoch: 1 step: 240, loss is 0.6885923147201538\n",
      "epoch: 1 step: 300, loss is 0.6849395632743835\n",
      "epoch: 1 step: 360, loss is 0.6889755129814148\n",
      "epoch: 1 step: 420, loss is 0.6838794946670532\n",
      "epoch: 1 step: 480, loss is 0.6855593919754028\n",
      "epoch: 1 step: 540, loss is 0.6798254251480103\n",
      "epoch: 1 step: 600, loss is 0.6766155958175659\n",
      "epoch: 1 step: 660, loss is 0.6806970834732056\n",
      "epoch: 1 step: 720, loss is 0.685994029045105\n",
      "epoch: 1 step: 780, loss is 0.6762064695358276\n",
      "epoch: 1 step: 840, loss is 0.6789414286613464\n",
      "epoch: 1 step: 900, loss is 0.6843880414962769\n",
      "epoch: 1 step: 960, loss is 0.6742408275604248\n",
      "epoch: 1 step: 1020, loss is 0.6833506226539612\n",
      "epoch: 1 step: 1080, loss is 0.6628250479698181\n",
      "epoch: 1 step: 1140, loss is 0.6638712882995605\n",
      "epoch: 1 step: 1200, loss is 0.6819545030593872\n",
      "epoch: 1 step: 1260, loss is 0.6634235382080078\n",
      "epoch: 1 step: 1320, loss is 0.6659316420555115\n",
      "epoch: 2 step: 10, loss is 0.6562868356704712\n",
      "epoch: 2 step: 70, loss is 0.6676459312438965\n",
      "epoch: 2 step: 130, loss is 0.6666393876075745\n",
      "epoch: 2 step: 190, loss is 0.6309773921966553\n",
      "epoch: 2 step: 250, loss is 0.6914771795272827\n",
      "epoch: 2 step: 310, loss is 0.6586712002754211\n",
      "epoch: 2 step: 370, loss is 0.6633787155151367\n",
      "epoch: 2 step: 430, loss is 0.6447461843490601\n",
      "epoch: 2 step: 490, loss is 0.6247007846832275\n",
      "epoch: 2 step: 550, loss is 0.6622743606567383\n",
      "epoch: 2 step: 610, loss is 0.6552753448486328\n",
      "epoch: 2 step: 670, loss is 0.6474553346633911\n",
      "epoch: 2 step: 730, loss is 0.6466821432113647\n",
      "epoch: 2 step: 790, loss is 0.6769295930862427\n",
      "epoch: 2 step: 850, loss is 0.6217220425605774\n",
      "epoch: 2 step: 910, loss is 0.6611166000366211\n",
      "epoch: 2 step: 970, loss is 0.6697641015052795\n",
      "epoch: 2 step: 1030, loss is 0.669894278049469\n",
      "epoch: 2 step: 1090, loss is 0.6619924306869507\n",
      "epoch: 2 step: 1150, loss is 0.6616406440734863\n",
      "epoch: 2 step: 1210, loss is 0.5906901359558105\n",
      "epoch: 2 step: 1270, loss is 0.6351121664047241\n",
      "epoch: 2 step: 1330, loss is 0.625430703163147\n",
      "epoch: 3 step: 20, loss is 0.6251931190490723\n",
      "epoch: 3 step: 80, loss is 0.662312388420105\n",
      "epoch: 3 step: 140, loss is 0.7272656559944153\n",
      "epoch: 3 step: 200, loss is 0.606147050857544\n",
      "epoch: 3 step: 260, loss is 0.6534096598625183\n",
      "epoch: 3 step: 320, loss is 0.6435432434082031\n",
      "epoch: 3 step: 380, loss is 0.6055505871772766\n",
      "epoch: 3 step: 440, loss is 0.6335690021514893\n",
      "epoch: 3 step: 500, loss is 0.6434898376464844\n",
      "epoch: 3 step: 560, loss is 0.6240215301513672\n",
      "epoch: 3 step: 620, loss is 0.6435309648513794\n",
      "epoch: 3 step: 680, loss is 0.6337924003601074\n",
      "epoch: 3 step: 740, loss is 0.6337907314300537\n",
      "epoch: 3 step: 800, loss is 0.6531808376312256\n",
      "epoch: 3 step: 860, loss is 0.6531294584274292\n",
      "epoch: 3 step: 920, loss is 0.6433488726615906\n",
      "epoch: 3 step: 980, loss is 0.5946968793869019\n",
      "epoch: 3 step: 1040, loss is 0.7212131023406982\n",
      "epoch: 3 step: 1100, loss is 0.6236332654953003\n",
      "epoch: 3 step: 1160, loss is 0.6239876747131348\n",
      "epoch: 3 step: 1220, loss is 0.6241594552993774\n",
      "epoch: 3 step: 1280, loss is 0.6527535915374756\n",
      "epoch: 3 step: 1340, loss is 0.7311028242111206\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "dataset_train = create_dataset(\"./mindrecord\", batch_size=batch_size, num_epochs=10, is_train=True)\n",
    "network = SentimentNet(vocab_size=embedding_tabel.shape[0],\n",
    "                embed_size=100,\n",
    "                num_hiddens=100,\n",
    "                num_layers=2,\n",
    "                bidirectional=False,\n",
    "                num_classes=2,\n",
    "                weight=Tensor(embedding_tabel),\n",
    "                batch_size=batch_size)\n",
    "\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits( sparse=True)\n",
    "opt = nn.Adam(network.trainable_params(), learning_rate=1e-5, weight_decay=1e-8)\n",
    "loss_callback = LossMonitor(per_print_times=60)\n",
    "model = Model(network, loss, opt, {'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=390, keep_checkpoint_max=10)\n",
    "checkpoint_cb = ModelCheckpoint(prefix=\"lstm\", directory=\"./model\", config=config_ck)\n",
    "\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"Ascend\")\n",
    "model.train(3, dataset_train, callbacks=[checkpoint_cb, loss_callback], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6725d-eff9-44b5-9c92-a549bb2bd806",
   "metadata": {},
   "source": [
    "æ¨¡åž‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f59f829-84e0-4bd5-b15d-c3baa445f3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:{'acc': 0.7079044117647059}\n"
     ]
    }
   ],
   "source": [
    "dataset_test = create_dataset(\"./mindrecord\", batch_size=32, num_epochs=10, is_train=False)\n",
    "acc = model.eval(dataset_test)\n",
    "print(\"accuracy:{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e696d2e7-f431-42b0-9606-0c11b3f40e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:{'acc': 0.6660845588235295}\n"
     ]
    }
   ],
   "source": [
    "dataset_test = create_dataset(\"./mindrecord\", batch_size=batch_size, num_epochs=10, is_train=False)\n",
    "acc = model.eval(dataset_test)\n",
    "print(\"accuracy:{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d416b1-bc3d-482c-9d50-2d8b61264108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test = create_dataset(\"./mindrecord\", batch_size=batch_size, num_epochs=10, is_train=False)\n",
    "# acc = model.eval(dataset_test)\n",
    "# print(\"accuracy:{:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36880a-8bed-40db-bb43-18c20d3e6752",
   "metadata": {},
   "source": [
    "æµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd154ea-25ba-40fe-84d6-372c57eb03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)\n",
    "dataset_real_test=create_dataset(\"./mindrecordtest\", batch_size=batch_size, num_epochs=10, is_train=True)\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    preds_list=[]\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    # model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model.predict(data).asnumpy()\n",
    "        preds = np.argmax(pred, axis=1)\n",
    "        preds_list.extend(preds)\n",
    "    return preds_list\n",
    "preds_list=test_loop(model,dataset_real_test,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747f5985-ce7d-4fb5-aa33-4cbcecb79d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_list).to_csv(r'.comment_result.txt', header=False, index=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc68a7c4-d1dd-4dc7-9ec4-eba4b5b49739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac7e59-4e94-41fc-b0a7-d0c4d0a5c3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384acfc1-dc7f-4ebf-a9ce-adf2cc1f4d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef7ebf-0b5a-4bc4-ac96-44d4ec20b357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949f04e-417a-46fe-b3a8-08168930abea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275453b-6e10-44aa-be38-a50cb945b203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
